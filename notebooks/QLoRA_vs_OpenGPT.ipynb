{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pB62adTBsvhk"
   },
   "source": [
    "## QLoRA vs OpenGPT on Colab\n",
    "\n",
    "⚠ VERY EXPERIMENTAL ⚠\n",
    "\n",
    "Initial attempts to train an LLM using QLoRA on a free T4 GPU on Colab along the lines of the OpenGPT approach.\n",
    "\n",
    "Borrowing heavily from and combining the following:\n",
    "- https://github.com/CogStack/OpenGPT\n",
    "- https://github.com/artidoro/qlora\n",
    "\n",
    "Major thanks to the developers of both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kpy9nDMBsYYw"
   },
   "outputs": [],
   "source": [
    "# **Make sure you have the runtime on GPU as QLoRA currently doesn't work without it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1110HHgtFOt"
   },
   "outputs": [],
   "source": [
    "# Install all the things!\n",
    "! pip install -U bitsandbytes\n",
    "! pip install -U git+https://github.com/huggingface/transformers.git\n",
    "! pip install -U git+https://github.com/huggingface/peft.git\n",
    "! pip install -U git+https://github.com/huggingface/accelerate.git\n",
    "\n",
    "! pip install -U opengpt\n",
    "\n",
    "! pip install einops xformers\n",
    "\n",
    "! pip install -U tokenizers\n",
    "! pip install -U protobuf==3.20.3\n",
    "! pip install -U sentencepiece\n",
    "\n",
    "import os\n",
    "\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cD6nYxAFt4Sa"
   },
   "outputs": [],
   "source": [
    "# Once packages installed - run from here\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "from opengpt.config import Config\n",
    "from opengpt.model_utils import add_tokens_to_model_and_tokenizer\n",
    "from opengpt.dataset_utils import create_labels, pack_examples\n",
    "from opengpt.data_collator import DataCollatorWithPadding\n",
    "\n",
    "import torch\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRhTxUAjt_4V"
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MztuUcv6uDqE"
   },
   "outputs": [],
   "source": [
    "# Download the configs and data from OpenGPT repo https://github.com/CogStack/OpenGPT\n",
    "\n",
    "! wget https://raw.githubusercontent.com/CogStack/OpenGPT/main/data/nhs_uk_full/prepared_generated_data_for_nhs_uk_qa.csv\n",
    "! wget https://raw.githubusercontent.com/CogStack/OpenGPT/main/data/nhs_uk_full/prepared_generated_data_for_nhs_uk_conversations.csv\n",
    "! wget https://raw.githubusercontent.com/CogStack/OpenGPT/main/data/medical_tasks_gpt4/prepared_generated_data_for_medical_tasks.csv\n",
    "! wget https://raw.githubusercontent.com/CogStack/OpenGPT/main/configs/example_train_config.yaml\n",
    "! wget https://raw.githubusercontent.com/CogStack/OpenGPT/main/data/example_project_data/prepared_generated_data_for_example_project.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yASj2lpxuTaq"
   },
   "outputs": [],
   "source": [
    "# Load the config - we actually don't use lots of it\n",
    "config = Config(yaml_path=\"./example_train_config.yaml\")\n",
    "\n",
    "# This config can be used as a template\n",
    "config.train.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5pJ_c5auTMu"
   },
   "outputs": [],
   "source": [
    "model_id = \"decapoda-research/llama-13b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsVMs1-Wvfmx"
   },
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUgjFjeTvsk4"
   },
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 1024  # config.train.max_seq_len\n",
    "# Can push this to 1024 according to table in blog:\n",
    "# https://huggingface.co/blog/4bit-transformers-bitsandbytes#what-other-consequences-are-there\n",
    "\n",
    "add_tokens_to_model_and_tokenizer(config, tokenizer, model)\n",
    "\n",
    "config.train.datasets = [\n",
    "    \"./prepared_generated_data_for_nhs_uk_qa.csv\",\n",
    "    \"./prepared_generated_data_for_nhs_uk_conversations.csv\",\n",
    "    \"./prepared_generated_data_for_medical_tasks.csv\",\n",
    "]\n",
    "\n",
    "# Load datasets and shuffle if needed\n",
    "train_dataset = datasets.Dataset.from_csv(config.train.datasets)\n",
    "if config.train.shuffle_dataset:\n",
    "    train_dataset = train_dataset.shuffle()\n",
    "    print(\"Shuffling dataset!\")\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SYlDsLuwgw7"
   },
   "outputs": [],
   "source": [
    "train_dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwJN6YA1wkBt"
   },
   "outputs": [],
   "source": [
    "to_remove = list(train_dataset.column_names)\n",
    "to_remove.remove(\"text\")\n",
    "train_dataset = train_dataset.remove_columns(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8_llZJ0wnIy"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDsWvo-qwqN6"
   },
   "outputs": [],
   "source": [
    "# Minor fix to create_labels (was required locally at least)\n",
    "def create_labels(examples, config, tokenizer):\n",
    "    r\"\"\"This is used with a prepared HF dataset that is already tokenized. It will add labels\n",
    "    so that only the AI generated parts (answers) will be trained on.\n",
    "    \"\"\"\n",
    "\n",
    "    user_token_id = tokenizer.get_vocab()[config.special_tokens.user]\n",
    "    ai_token_id = tokenizer.get_vocab()[config.special_tokens.ai]\n",
    "    # Everything written by an AI will be used for training, and everything by a user will be ignored\n",
    "\n",
    "    examples[\"labels\"] = []\n",
    "    for i in range(len(examples[\"input_ids\"])):\n",
    "        labels = []\n",
    "        ignore = True\n",
    "        for tkn_id in examples[\"input_ids\"][i]:\n",
    "            if tkn_id == user_token_id:\n",
    "                ignore = True\n",
    "            elif tkn_id == ai_token_id:\n",
    "                ignore = False\n",
    "\n",
    "            if ignore:\n",
    "                labels.append(config.train.ignore_index)\n",
    "            else:\n",
    "                labels.append(tkn_id)\n",
    "        examples[\"labels\"].append(labels)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRNA0NVEw8mZ"
   },
   "outputs": [],
   "source": [
    "# Ignore max_seq_len warning, it is handled by the packer or data_collator\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda examples: tokenizer(examples[\"text\"], add_special_tokens=False),\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oixjiBbw_jP"
   },
   "outputs": [],
   "source": [
    "# Create labels for supervised training (meaning we do not train on questions, but only on answers)\n",
    "\n",
    "# Llama Temp Fix in create_labels function\n",
    "# user_token_id = tokenizer.get_vocab()[config.special_tokens.user]\n",
    "# ai_token_id = tokenizer.get_vocab()[config.special_tokens.ai]\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda examples: create_labels(examples, config, tokenizer),\n",
    "    batched=True,\n",
    "    batch_size=1_000,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXRgbUJtxC1o"
   },
   "outputs": [],
   "source": [
    "# We only do packing for the train set\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda examples: pack_examples(\n",
    "        examples, config.train.max_seq_len, packing_type=config.train.packing_type\n",
    "    ),\n",
    "    batched=True,\n",
    "    batch_size=1_000,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3WULO10xFZZ"
   },
   "outputs": [],
   "source": [
    "# Check the new train_dataset (take note of how the labels look). \n",
    "# The USER (Question) part of the input should have a label of -100,\n",
    "# and the AI part (Answer) should have labels equal to input_ids\n",
    "for i in range(50):\n",
    "    print(\n",
    "        train_dataset[0][\"input_ids\"][i],\n",
    "        train_dataset[0][\"labels\"][i],\n",
    "        train_dataset[0][\"attention_mask\"][i],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfiaKc5ZxTxd"
   },
   "outputs": [],
   "source": [
    "# PEFT apply\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWOuGmvzxaYI"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        (\n",
    "            f\"trainable params: {trainable_params} || \"\n",
    "            f\"all params: {all_param} || trainable %: \"\n",
    "            f\"{100 * trainable_params / all_param}\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvZIKYVS2yYB"
   },
   "outputs": [],
   "source": [
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YMjpvJ1212I"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",  # Should we include this?\n",
    "    ],  # [\"query_key_value\"],  # [\"k_proj\", \"v_proj\", \"q_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zJpjryz3Gjk"
   },
   "outputs": [],
   "source": [
    "# needed for gpt-neo-x tokenizer in demo - do we need this for Llama?\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dc = DataCollatorWithPadding(\n",
    "    tokenizer.pad_token_id,\n",
    "    config.train.ignore_index,\n",
    "    max_seq_len=config.train.max_seq_len,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        # warmup_steps=2,\n",
    "        max_steps=1_000,\n",
    "        learning_rate=1e-4,  # 3e-4,  # 1.2e-4,  # 2e-4,\n",
    "        # lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=1,\n",
    "        # save_strategy=\"steps\",\n",
    "        # save_steps=250,\n",
    "        seed=11,\n",
    "        # num_train_epochs=1,\n",
    "        fp16=True,\n",
    "        max_grad_norm=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        # load_best_model_at_end=True,\n",
    "    ),\n",
    "    data_collator=dc,  # DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur3Yq-Kk3u2N"
   },
   "source": [
    "## Eval on some OpenGPT examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWPpo5FZ4Qls"
   },
   "outputs": [],
   "source": [
    "def responder(t):\n",
    "    \"\"\"Take an input and respond\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(t, return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids, do_sample=True, max_length=128, temperature=0.2\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**generate_kwargs)\n",
    "\n",
    "    return tokenizer.batch_decode(outputs.detach().cpu().numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nP7Qx5vX3TL9"
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = True  # Re-enable\n",
    "\n",
    "model.eval()\n",
    "\n",
    "t = \"<|user|> What is diabetes? <|eos|> <|ai|>\"  # The format with special tokens is required, because of training\n",
    "\n",
    "print(responder(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oW9yqpLw4La4"
   },
   "outputs": [],
   "source": [
    "t = \"<|user|> What is vitamin d3 and should I take it? <|eos|> <|ai|>\"\n",
    "\n",
    "print(responder(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0EIDhM34ryF"
   },
   "outputs": [],
   "source": [
    "t = \"<|user|> What is HTN? <|eos|> <|ai|>\"\n",
    "\n",
    "print(responder(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJrcf0Dg-7S8"
   },
   "outputs": [],
   "source": [
    "t = \"<|user|> What is the capital of France? <|eos|> <|ai|>\"\n",
    "\n",
    "print(responder(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKOG4ye-_HeJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
